{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pypdf in ./venv/lib/python3.10/site-packages (3.15.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: chromadb in ./venv/lib/python3.10/site-packages (0.4.9)\r\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in ./venv/lib/python3.10/site-packages (from chromadb) (0.99.1)\r\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in ./venv/lib/python3.10/site-packages (from chromadb) (1.10.12)\r\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.10/site-packages (from chromadb) (0.48.9)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.10/site-packages (from chromadb) (4.7.1)\r\n",
      "Requirement already satisfied: numpy>=1.22.5 in ./venv/lib/python3.10/site-packages (from chromadb) (1.24.4)\r\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in ./venv/lib/python3.10/site-packages (from chromadb) (3.3.0)\r\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in ./venv/lib/python3.10/site-packages (from chromadb) (0.23.2)\r\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.2 in ./venv/lib/python3.10/site-packages (from chromadb) (0.7.2)\r\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.10/site-packages (from chromadb) (1.15.1)\r\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./venv/lib/python3.10/site-packages (from chromadb) (4.0.1)\r\n",
      "Requirement already satisfied: requests>=2.28 in ./venv/lib/python3.10/site-packages (from chromadb) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.10/site-packages (from chromadb) (0.13.3)\r\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.10/site-packages (from chromadb) (6.0.1)\r\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./venv/lib/python3.10/site-packages (from chromadb) (3.0.2)\r\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./venv/lib/python3.10/site-packages (from chromadb) (4.66.1)\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.10/site-packages (from chromadb) (7.4.0)\r\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./venv/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\r\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.3)\r\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\r\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\r\n",
      "Requirement already satisfied: monotonic>=1.5 in ./venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\r\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>2.1 in ./venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\r\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.28->chromadb) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.2.0)\r\n",
      "Requirement already satisfied: click>=7.0 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\r\n",
      "Requirement already satisfied: h11>=0.8 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\r\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\r\n",
      "Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\r\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\r\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\r\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./venv/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\r\n",
      "Requirement already satisfied: exceptiongroup in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: langchain in ./venv/lib/python3.10/site-packages (0.0.285)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./venv/lib/python3.10/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./venv/lib/python3.10/site-packages (from langchain) (0.5.14)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./venv/lib/python3.10/site-packages (from langchain) (8.2.3)\r\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./venv/lib/python3.10/site-packages (from langchain) (2.8.5)\r\n",
      "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.10/site-packages (from langchain) (2.31.0)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.10/site-packages (from langchain) (2.0.20)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in ./venv/lib/python3.10/site-packages (from langchain) (1.24.4)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.10/site-packages (from langchain) (6.0.1)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.10/site-packages (from langchain) (3.8.5)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./venv/lib/python3.10/site-packages (from langchain) (1.10.12)\r\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in ./venv/lib/python3.10/site-packages (from langchain) (0.0.35)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\r\n",
      "Requirement already satisfied: packaging>=17.0 in ./venv/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.10/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.33.1)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (2.0.1)\r\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.15.2)\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.24.4)\r\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.3.0)\r\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.11.2)\r\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\r\n",
      "Requirement already satisfied: sentencepiece in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.16.4)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\r\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\r\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.0.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: faiss-cpu in ./venv/lib/python3.10/site-packages (1.7.4)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n",
    "!pip install chromadb\n",
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T10:19:15.134144Z",
     "start_time": "2023-09-09T10:19:08.023046Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: llama-cpp-python in ./venv/lib/python3.10/site-packages (0.1.83)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (1.24.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (4.7.1)\r\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./venv/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T10:19:16.747252Z",
     "start_time": "2023-09-09T10:19:15.135887Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm_ll2 = Ollama(base_url=\"http://localhost:11434\",\n",
    "                 model=\"llama2\",\n",
    "                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T10:19:16.751472Z",
     "start_time": "2023-09-09T10:19:16.749152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T10:19:16.754256Z",
     "start_time": "2023-09-09T10:19:16.751714Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model_path = '/Users/jamakase/Downloads/ggml-model-q4_K.gguf'\n",
    "# from llama_cpp import Llama\n",
    "# llm = Llama(model_path=model_path)\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T10:19:16.758445Z",
     "start_time": "2023-09-09T10:19:16.756063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from /Users/jamakase/Downloads/ggml-model-q4_K.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.32.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:             blk.32.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:        blk.32.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:           blk.32.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:             blk.32.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.33.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:             blk.33.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:        blk.33.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:           blk.33.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:             blk.33.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.34.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:             blk.34.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:        blk.34.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:           blk.34.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:             blk.34.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:             blk.35.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:             blk.35.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:        blk.35.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:           blk.35.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:             blk.35.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:             blk.36.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:             blk.36.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:        blk.36.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:           blk.36.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:             blk.36.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.37.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:             blk.37.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:        blk.37.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:           blk.37.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:             blk.37.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:             blk.38.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:             blk.38.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:        blk.38.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:           blk.38.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:             blk.38.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:             blk.39.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:             blk.39.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_v.weight q6_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:        blk.39.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:           blk.39.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:             blk.39.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 2048\n",
      "llm_load_print_meta: n_ctx          = 2048\n",
      "llm_load_print_meta: n_embd         = 5120\n",
      "llm_load_print_meta: n_head         = 40\n",
      "llm_load_print_meta: n_head_kv      = 40\n",
      "llm_load_print_meta: n_layer        = 40\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 1\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 1.0e-05\n",
      "llm_load_print_meta: n_ff           = 13824\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 13B\n",
      "llm_load_print_meta: model ftype    = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model size     = 13.02 B\n",
      "llm_load_print_meta: general.name   = .\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MB\n",
      "llm_load_tensors: mem required  = 7500.97 MB (+ 1600.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/jamakase/Projects/banzai-team/rzd-assistant/ml/venv/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x2b35df300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                        0x2aa9c0700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                            0x2b3d57b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x2b3a82b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                          0x2b3b4adf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                           0x2b0ad4890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                           0x2aa943dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                           0x2b3d569d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x2b3a82ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x2b3d55fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x2b3d5b6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x2b08ff080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x2b0ad4f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                  0x2b3d5bd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x2b3d5c560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x2b0ad5480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x2b0ad59b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x2b3d5ca20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x2b35de850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x2b3a83700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                           0x2b0ad5ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x2b3b4b8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x2b3a83f20 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x2b3d5d670 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x2b0ad6820 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x2b3d5dc50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x2b08ff780 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x2b3a84570 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x2b3d5e0f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x2c8704080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x2b35db590 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x2b0ad6f30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x2aa94da90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x2b3b4b690 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x2b3d5ef10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x2b3a84110 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x2b35dd080 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x2b35eaba0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x2b0ad7550 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope                           0x2b3d5ec50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x2b3d5fee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x2b3a84fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x2b0ad7f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x2b3d5fb90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =  211.47 MB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7501.56 MB, ( 7502.00 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.48 MB, ( 7503.48 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1602.00 MB, ( 9105.48 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   210.02 MB, ( 9315.50 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T10:19:17.882671Z",
     "start_time": "2023-09-09T10:19:16.759655Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader, CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedder_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedder_name)\n",
    "\n",
    "loader = TextLoader(file_path='test.txt')\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\n",
    "\n",
    "SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "SYSTEM_TOKEN = 1788\n",
    "USER_TOKEN = 1404\n",
    "BOT_TOKEN = 9225\n",
    "LINEBREAK_TOKEN = 13\n",
    "\n",
    "ROLE_TOKENS = {\n",
    "    \"user\": USER_TOKEN,\n",
    "    \"bot\": BOT_TOKEN,\n",
    "    \"system\": SYSTEM_TOKEN\n",
    "}\n",
    "\n",
    "\n",
    "def get_message_tokens(model, role, content):\n",
    "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
    "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
    "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
    "    message_tokens.append(model.token_eos())\n",
    "    return message_tokens\n",
    "\n",
    "\n",
    "def get_system_tokens(model):\n",
    "    system_message = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "    return get_message_tokens(model, **system_message)\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if len(line.strip()) > 2]\n",
    "    text = \"\\n\".join(lines).strip()\n",
    "    if len(text) < 10:\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "\n",
    "splitted_docs = text_splitter.split_documents(documents)\n",
    "fixed_documents = []\n",
    "for doc in splitted_docs:\n",
    "    doc.page_content = process_text(doc.page_content)\n",
    "    if not doc.page_content:\n",
    "        continue\n",
    "    fixed_documents.append(doc)\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "\n",
    "docsearch = Chroma.from_documents(fixed_documents, embeddings)\n",
    "# retriever = docsearch.as_retriever()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T10:19:23.634136Z",
     "start_time": "2023-09-09T10:19:17.885495Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve(history, db, retrieved_docs, k_documents):\n",
    "    if db:\n",
    "        msg = history[-1][0]\n",
    "        retriever = db.as_retriever(search_kwargs={\"k\": k_documents})\n",
    "        docs = retriever.get_relevant_documents(msg)\n",
    "        retrieved_docs = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    return retrieved_docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "' bot\\nЕсли при установке штурвала контролера на первую позицию тепловоз с места не трогается, то необходимо провести диагностику силовой цепи и регулятора дизеля. Если вы обнаружите повреждение в силовой цепи или нарушение привода объединенного регулятора дизеля, следует следовать инструкции по устранению неисправности. Если же проблема связана с реле заземления, то необходимо провести дополнительную диагностику и устранить причину возникновения сигнала о повреждении.'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос пользователя: При установке штурвала контролера на первую позицию тепловоз с места не трогается; Возможная неисправность:Снимается нагрузка, но дополнительно включается сигнальная лампа \"Реле заземления\".; Вероятная причина: Сработало реле заземления. В силовой цепи - пробой на корпус или частичное разрушение изоляции.; Метод устранения: Осмотрите всю силовую цепь. Если повреждение не\n",
      "\n",
      "Запрос пользователя: При установке штурвала контролера на первую позицию тепловоз с места не трогается; Возможная неисправность:Снимается нагрузка, но дополнительно включается сигнальная лампа \"Реле заземления\".; Вероятная причина: Сработало реле заземления. В силовой цепи - пробой на корпус или частичное разрушение изоляции.; Метод устранения: Осмотрите всю силовую цепь. Если повреждение не\n",
      "\n",
      "Запрос пользователя: При установке штурвала контролера на первую позицию тепловоз с места не трогается; Возможная неисправность:Тепловоз приходит в движение, но по кА и кВ наблюдается обратная полярность.; Вероятная причина: Обрыв цепи независимой обмотки возбуждения СПВ, обрыв ремней привода, зависание щёток, загрязнения колец.; Метод устранения: Перейти на аварийное возбуждение.\n",
      "\n",
      "Запрос пользователя: При установке штурвала контролера на первую позицию тепловоз с места не трогается; Возможная неисправность:Тепловоз приходит в движение, но по кА и кВ наблюдается обратная полярность.; Вероятная причина: Обрыв цепи независимой обмотки возбуждения СПВ, обрыв ремней привода, зависание щёток, загрязнения колец.; Метод устранения: Перейти на аварийное возбуждение.\n",
      "\n",
      "Запрос пользователя: При установке штурвала контролера на первую позицию тепловоз с места не трогается; Возможная неисправность:Дизель идет вразнос (резко увеличивается частота вращения коленчатого вала).; Вероятная причина: Нарушен привод объединённого регулятора дизеля.; Метод устранения: Следовать на одной секции.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": "' bot\\nЕсли при установке штурвала контролера на первую позицию тепловоз с места не трогается, то возможно несколько причин. Сначала стоит проверить силовую цепь и убедиться в отсутствии повреждений. Если это не помогло, то следует обратить внимание на реле заземления. Возможно, оно сработало из-за пробоя на корпус или частичного разрушения изоляции. В этом случае необходимо провести ремонт силовой цепи и заменить поврежденные элементы. Если проблема не в силовой цепи, то следует обратиться к специалистам для диагностики и устранения неисправностей.'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''Как мне быть, если при установки штурвала контроллера на первую позицию тепловоз не едет'''\n",
    "\n",
    "# retriever.get_relevant_documents(query)\n",
    "\n",
    "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "# qa.run(query)\n",
    "\n",
    "model = llm\n",
    "\n",
    "max_new_tokens = 1500\n",
    "chat_history = [[query]]\n",
    "retireved_docs = retrieve(chat_history, docsearch, [], 5)\n",
    "print(retireved_docs)\n",
    "\n",
    "last_user_message=chat_history[-1][0]\n",
    "if retireved_docs:\n",
    "    last_user_message = f\"Контекст: {retireved_docs}\\n\\nИспользуя контекст, ответь на вопрос: {last_user_message}\"\n",
    "tokens = get_system_tokens(model)[:]\n",
    "tokens.append(LINEBREAK_TOKEN)\n",
    "role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
    "tokens.extend(role_tokens)\n",
    "\n",
    "generator = model.generate(\n",
    "    tokens,\n",
    "    top_k=30,\n",
    "    top_p=0.9,\n",
    "    temp=0.1\n",
    ")\n",
    "\n",
    "message_tokens = get_message_tokens(model=model, role=\"user\", content=last_user_message)\n",
    "tokens.extend(message_tokens)\n",
    "\n",
    "partial_text = \"\"\n",
    "for i, token in enumerate(generator):\n",
    "    if token == model.token_eos() or (max_new_tokens is not None and i >= max_new_tokens):\n",
    "        break\n",
    "    partial_text += model.detokenize([token]).decode(\"utf-8\", \"ignore\")\n",
    "    # chat_history[-1][1] = partial_text\n",
    "    # yield history\n",
    "\n",
    "partial_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
